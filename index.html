<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><script type="importmap">{"imports":{"#entry":"/_nuxt/CGCySwaJ.js"}}</script><title>Yunhao Li | Yunhao Li</title><link rel="stylesheet" href="/_nuxt/entry.Co2T8a3r.css" crossorigin><link rel="stylesheet" href="/_nuxt/default.CQL9LdGM.css" crossorigin><link rel="stylesheet" href="/_nuxt/style.BHgrWUtD.css" crossorigin><link rel="stylesheet" href="/_nuxt/style.CKTNkHcw.css" crossorigin><link rel="stylesheet" href="/_nuxt/style.BFI6Bq-W.css" crossorigin><link rel="stylesheet" href="/_nuxt/app.BydcJ6zn.css" crossorigin><link rel="stylesheet" href="/_nuxt/style.DMa4Mjx6.css" crossorigin><link rel="stylesheet" href="/_nuxt/pages.Da9Xq7zS.css" crossorigin><style>:where(.i-fa6-brands\:google-scholar){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 512 512' width='512' height='512'%3E%3Cpath fill='black' d='M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64c1.7-3.6 3.6-7.2 5.6-10.7q6.6-11.4 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3c33.6 0 64.6 11.1 89.6 29.9c9.1 6.9 17.4 14.7 24.8 23.5c5.6 6.6 10.6 13.8 15 21.3c2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7L256 0l256 202.7l-94.7 77.1z'/%3E%3C/svg%3E")}:where(.i-fa6-brands\:orcid){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 512 512' width='512' height='512'%3E%3Cpath fill='black' d='M294.75 188.19h-45.92V342h47.47c67.62 0 83.12-51.34 83.12-76.91c0-41.64-26.54-76.9-84.67-76.9M256 8C119 8 8 119 8 256s111 248 248 248s248-111 248-248S393 8 256 8m-80.79 360.76h-29.84v-207.5h29.84zm-14.92-231.14a19.57 19.57 0 1 1 19.57-19.57a19.64 19.64 0 0 1-19.57 19.57M300 369h-81V161.26h80.6c76.73 0 110.44 54.83 110.44 103.85C410 318.39 368.38 369 300 369'/%3E%3C/svg%3E")}:where(.i-fa6-solid\:blog){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 512 512' width='512' height='512'%3E%3Cpath fill='black' d='M192 32c0 17.7 14.3 32 32 32c123.7 0 224 100.3 224 224c0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32c70.7 0 128 57.3 128 128c0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192c-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144s-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48s-48-21.5-48-48z'/%3E%3C/svg%3E")}:where(.i-garden\:github-fill-16){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' width='16' height='16'%3E%3Cpath fill='black' d='M8 0C3.582 0 0 3.672 0 8.203c0 3.624 2.292 6.698 5.471 7.783c.4.075.546-.178.546-.396c0-.194-.006-.71-.01-1.395c-2.226.496-2.695-1.1-2.695-1.1c-.364-.947-.888-1.199-.888-1.199c-.727-.509.055-.499.055-.499c.803.059 1.225.846 1.225.846c.713 1.253 1.872.89 2.328.681c.073-.53.28-.891.508-1.096c-1.776-.207-3.644-.911-3.644-4.054c0-.896.312-1.628.824-2.201c-.083-.208-.357-1.042.078-2.171c0 0 .671-.22 2.2.84A7.5 7.5 0 0 1 8 3.967c.68.004 1.364.095 2.003.277c1.528-1.062 2.198-.841 2.198-.841c.436 1.13.162 1.963.08 2.17c.512.574.822 1.306.822 2.202c0 3.15-1.87 3.844-3.653 4.047c.288.253.543.754.543 1.52c0 1.096-.01 1.98-.01 2.25c0 .219.144.474.55.394C13.71 14.898 16 11.825 16 8.203C16 3.673 12.418 0 8 0'/%3E%3C/svg%3E")}:where(.i-hugeicons\:search-01){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='1.5' d='m17 17l4 4m-2-10a8 8 0 1 0-16 0a8 8 0 0 0 16 0'/%3E%3C/svg%3E")}:where(.i-lsicon\:file-pdf-outline){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' width='16' height='16'%3E%3Cpath fill='none' stroke='black' stroke-linejoin='round' d='M4.5 12v-2m0 0V7.5H7V10zM15 7.5h-2.5V10m0 2v-2m0 0H15m-6.5 1.5H11v-4H8.5m0 4.5V7m3 6.5v1h-9v-13h6m0 0v3h3m-3-3H9L11.5 4v.5m0 0V6'/%3E%3C/svg%3E")}:where(.i-material-symbols\:light-mode){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='black' d='M12 17q-2.075 0-3.537-1.463T7 12t1.463-3.537T12 7t3.538 1.463T17 12t-1.463 3.538T12 17m-7-4H1v-2h4zm18 0h-4v-2h4zM11 5V1h2v4zm0 18v-4h2v4zM6.4 7.75L3.875 5.325L5.3 3.85l2.4 2.5zm12.3 12.4l-2.425-2.525L17.6 16.25l2.525 2.425zM16.25 6.4l2.425-2.525L20.15 5.3l-2.5 2.4zM3.85 18.7l2.525-2.425L7.75 17.6l-2.425 2.525z'/%3E%3C/svg%3E")}:where(.i-material-symbols\:mail){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='black' d='M4 20q-.825 0-1.412-.587T2 18V6q0-.825.588-1.412T4 4h16q.825 0 1.413.588T22 6v12q0 .825-.587 1.413T20 20zm8-7l8-5V6l-8 5l-8-5v2z'/%3E%3C/svg%3E")}</style><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CGCySwaJ.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/U725BVvE.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DpYxfLlx.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CfYLUF7N.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Dae_vkcH.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/B6Qydxqy.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BZ0AlPGr.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DzvNXTjd.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BHonhOiW.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DP3yFTrx.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DShArktz.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Cl-EDz-4.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CaL0YeoM.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C326c92g.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DC4hiPh_.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DI4EpC9N.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BvQhSLs_.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/tnXUFGmR.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BxBQSOEL.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C_oamPMK.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DR2ko6O7.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/bb85fc13-cd4e-4b6a-a07d-f81f8017cc1b.json"><link rel="prefetch" as="style" crossorigin href="/_nuxt/mobile.CWbb_b09.css"><link rel="prefetch" as="script" crossorigin href="/_nuxt/wZdYlb_V.js"><link rel="prefetch" as="style" crossorigin href="/_nuxt/error-404.BQMs95bW.css"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BDamjP1D.js"><link rel="prefetch" as="style" crossorigin href="/_nuxt/error-500.xnE7cslF.css"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CscgXzQx.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DCcOR3jr.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/D-MSRrjw.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Bnv7B3D0.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B-FotuDE.js"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="title" content="Yunhao Li"><meta name="description" content="Blog of Yunhao Li, a passionate researcher who love to use programming skills and creativity to solve problems."><meta property="og:title" content="Yunhao Li"><meta property="og:image" content="https://yunhaoli.top/profile.JPG"><meta property="og:url" content="https://yunhaoli.top"><meta name="twitter:title" content="Yunhao Li"><meta name="twitter:image" content="https://yunhaoli.top/profile.JPG"><meta name="twitter:creator" content="@YunhaoLi"><meta property="og:type" content="website"><meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta property="og:description" content="Blog of Yunhao Li, a passionate researcher who love to use programming skills and creativity to solve problems."><meta name="twitter:description" content="Blog of Yunhao Li, a passionate researcher who love to use programming skills and creativity to solve problems."><meta name="twitter:card" content="summary"><link rel="icon" type="image/x-icon" href="/favicon.ico"><script type="module" src="/_nuxt/CGCySwaJ.js" crossorigin></script><link rel="canonical" href="https://yunhaoli.top/"><meta property="og:site_name" content="Yunhao Li"><script id="unhead:payload" type="application/json">{"title":"Yunhao Li"}</script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><div><div></div><div data-v-5dfd6732><div data-v-5dfd6732><section class="el-container is-vertical" data-v-5dfd6732><!--[--><header class="el-header h-60px" style="" data-v-5dfd6732><!--[--><ul role="menubar" style="--el-menu-level:0;" class="el-menu el-menu--horizontal" data-v-5dfd6732 data-v-1e26f161><li class="el-menu-item" role="menuitem" tabindex="-1" data-v-1e26f161><!--[--><!--[--> Home <!--]--><!--[--><!--]--><!--]--></li><li class="el-menu-item" role="menuitem" tabindex="-1" data-v-1e26f161><!--[--><!--[--> Blog <!--]--><!--[--><!--]--><!--]--></li><div class="flex-grow" data-v-1e26f161></div><li class="el-menu-item is-active" role="menuitem" tabindex="-1" style="display:none;" data-v-1e26f161 data-v-1e26f161><!--[--><!--[--><div class="flex items-center" data-v-1e26f161 data-v-d7fec013><button ariadisabled="false" type="button" class="el-button is-round tracking-wider font-medium shadow-black/8 shadow-sm transition-all duration-300 hover:shadow-black/12 hover:shadow-md hover:scale-105" style="" data-v-d7fec013><i class="el-icon" style=""><!--[--><span class="iconify i-hugeicons:search-01" aria-hidden="true" style="" data-v-1e26f161></span><!--]--></i><span class=""><!--[--> Search <!--]--></span></button><!--[--><div class="el-overlay" style="z-index:2001;display:none;"><!--[--><div role="dialog" aria-modal="true" aria-labelledby="el-id-1024-1" aria-describedby="el-id-1024-2" class="el-overlay-dialog" style=""><!--[--><!--]--></div><!--]--></div><!--]--></div><!--]--><!--[--><!--]--><!--]--></li><li class="el-menu-item is-active" role="menuitem" tabindex="-1" data-v-1e26f161><!--[--><!--[--><div class="el-switch dark:[--el-switch-on-color] light:[--el-switch-off-color]" data-v-1e26f161><input class="el-switch__input" type="checkbox" role="switch" aria-checked="false" aria-disabled="false" name true-value="true" false-value="false"><!--v-if--><span class="el-switch__core" style="width:;"><div class="el-switch__inner"><!--v-if--></div><div class="el-switch__action"><!--[--><i class="el-icon" style=""><!--[--><span class="iconify i-material-symbols:light-mode" aria-hidden="true" style="" data-v-5dfd6732></span><!--]--></i><!--]--></div></span><!--v-if--></div><!--]--><!--[--><!--]--><!--]--></li></ul><!--]--></header><main class="el-main h-[calc(100vh-60px)]" data-v-5dfd6732><!--[--><div class="el-scrollbar" data-v-5dfd6732><div class="el-scrollbar__wrap el-scrollbar__wrap--hidden-default" style=""><div class="el-scrollbar__view" style=""><!--[--><div class="el-col el-col-18 el-col-offset-3" style="" data-v-5dfd6732><!--[--><!--[--><div><div><div><h1 class="text-2xl font-bold mb-4"> 🌟 About </h1><div class="el-row" style="margin-left:-10px;margin-right:-10px;"><!--[--><div class="el-col el-col-8 is-guttered" style="padding-right:10px;padding-left:10px;"><!--[--><div class="flex justify-center"><div class="el-image max-w-[200px] w-full"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div></div><!--]--></div><div class="el-col el-col-16 is-guttered" style="padding-right:10px;padding-left:10px;"><!--[--><div><div class="text-base indent-8"><p> Hello, my name is Yunhao Li, and I am currently serving as a research assistant at Hong Kong University of Science and Technology, where I am fortunate to work under the esteemed guidance of <a class="text-[--el-color-primary]" href="https://people.ucas.ac.cn/~0058478">Prof. Qiong Wang</a>. My academic journey commenced at Guangzhou University, where I pursued and obtained my Master’s degree in Artificial Intelligence. During this enriching period, I was mentored by <a class="text-[--el-color-primary]" href="https://pangyan.me/">Assoc. Prof. Yan Pang </a>, whose invaluable insights and expertise have profoundly shaped my perspective and skills in the field. Prior to my Master’s degree, I also completed my Bachelor’s degree in Computer Science at Guilin University of Electronic Technology, further solidifying my foundation in the realm of technology. </p><p> My research interests focused on Computer Vision, Medical Image Analysis, and Multimodal Learning. I am deeply committed to harnessing the power of these technologies to make meaningful impact on the world. My ultimate aspiration is to contribute to the medical science, thereby enhancing human health. </p></div><div class="el-space el-space--horizontal float-right" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://github.com/yunhaoli24" target="_blank" rel="noopener noreferrer"><span class="iconify i-garden:github-fill-16 c-[--el-color-primary]" aria-hidden="true" style="font-size:28px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://scholar.google.com/citations?user=TDnI7j8AAAAJ" target="_blank" rel="noopener noreferrer"><span class="iconify i-fa6-brands:google-scholar c-[--el-color-primary]" aria-hidden="true" style="font-size:28px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="mailto: li.yunhao@foxmail.com" target="_blank" rel="noopener noreferrer"><span class="iconify i-material-symbols:mail c-[--el-color-primary]" aria-hidden="true" style="font-size:28px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://orcid.org/0000-0003-1704-3050" target="_blank" rel="noopener noreferrer"><span class="iconify i-fa6-brands:orcid c-[--el-color-primary]" aria-hidden="true" style="font-size:28px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="/blog" target="_blank" rel="noopener noreferrer"><span class="iconify i-fa6-solid:blog c-[--el-color-primary]" aria-hidden="true" style="font-size:28px;"></span></a><!--]--></div></div></div><!--]--></div><!--]--></div></div><div class="el-divider el-divider--horizontal" style="--el-border-style:solid;" role="separator"><!--v-if--></div><div><h1 class="text-2xl font-bold mb-4"> 📚 Publications </h1><ul class="el-timeline"><!--[--><!--[--><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2025</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">SegTom: A 3D Volumetric Medical Image Segmentation Framework for Thoracoabdominal Multi-Organ Anatomical Structures</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[-->Yan Pang, <strong><!--[-->Yunhao Li<!--]--></strong>, Jiaming Liang, Hao Chen, Ying Hu, Qiong Wang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">Journal of Biomedical and Health Informatics</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://ieeexplore.ieee.org/document/11151753" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://github.com/deepang-ai/SegTom" target="_blank"><span class="iconify i-garden:github-fill-16 c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->JBHI<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2025</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">Efficient Breast Lesion Segmentation from Ultrasound Videos Across Multiple Source-limited Platforms</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[-->Yan Pang, <strong><!--[-->Yunhao Li<!--]--></strong>, Teng Huang, Jiaming Liang, Ziyu Ding, Hao Chen, Baoliang Zhao, Ying Hu, Zheng Zhang, Qiong Wang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">Journal of Biomedical and Health Informatics</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://ieeexplore.ieee.org/document/10892059" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://github.com/deepang-ai/BaS" target="_blank"><span class="iconify i-garden:github-fill-16 c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->JBHI<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2025</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">Online Self-distillation and Self-modeling for 3D Brain Tumor Segmentation</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[-->Yan Pang, <strong><!--[-->Yunhao Li<!--]--></strong>, Teng Huang, Jiaming Liang, Zhen Wang, Changyu Dong, Dongyang Kuang, Ying Hu, Hao Chen, Tim Lei, Qiong Wang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">Journal of Biomedical and Health Informatics</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://ieeexplore.ieee.org/document/10843341" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://github.com/deepang-ai/MOD" target="_blank"><span class="iconify i-garden:github-fill-16 c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->JBHI<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2024</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">Optimized Breast Lesion Segmentation in Ultrasound Videos Across Varied Resource-Scant Environments</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[--><strong><!--[-->Yunhao Li<!--]--></strong>, Chen Zibin, Yan Junming, Ding Ziyu, Li Jie ,Pei Xiaoqing, Zhang Zheng, Qiong Wang, Yan Pang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">Asian Conference on Computer Vision</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://openaccess.thecvf.com/content/ACCV2024/papers/Li_Optimized_Breast_Lesion_Segmentation_in_Ultrasound_Videos_Across_Varied_Resource-Scant_ACCV_2024_paper.pdf" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->ACCV<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2024</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">IPM: An Intelligent Component for 3D Brain Tumor Segmentation Integrating Semantic Extractor and Pixel Refiner</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[--><strong><!--[-->Yunhao Li<!--]--></strong>, Caiyan Tan, Mingdu Zhang, Xi Zhang, Teng Huang ,Xiao-Qing Pei, Yan Pang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">Chinese Conference on Pattern Recognition and Computer Vision</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://link.springer.com/chapter/10.1007/978-981-97-8499-8_16" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->PRCV<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2023</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">Slim UNETR: Scale hybrid transformers to efficient 3D medical image segmentation under limited computational resources</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[-->Yan Pang, Jiaming Liang, Teng Huang, Hao Chen, <strong><!--[-->Yunhao Li<!--]--></strong>, Dan Li, Lin Huang, Qiong Wang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">IEEE Transactions on Medical Imaging</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://ieeexplore.ieee.org/abstract/document/10288609/" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://github.com/deepang-ai/Slim-UNETR" target="_blank"><span class="iconify i-garden:github-fill-16 c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->TMI<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2023</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">CAM-PC: A novel method for camouflaging point clouds to counter adversarial deception in remote sensing</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[-->Bo Wei, Teng Huang, Xi Zhang, Jiaming Liang, <strong><!--[-->Yunhao Li<!--]--></strong>, Cong Cao, Dan Li, Yongfeng Chen, Huagang Xiong, Feng Jiang, Xiqiu Zhang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://ieeexplore.ieee.org/abstract/document/10285331/" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->IEEE J-STARS<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2023</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><!--v-if--><div class="el-card__body" style=""><!--[--><div class="el-space el-space--horizontal flex-col-reverse md:flex-row" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><div class="el-image h-100% w-100% md:h-[200px] md:w-[300px]"><!--[--><!--v-if--><div class="el-image__wrapper"><!--[--><div class="el-image__placeholder"></div><!--]--></div><!--]--><!--v-if--></div><!--]--></div><div class="el-space__item" style=""><!--[--><div><div class="text-xl font-bold">AgileNet: A Rapid and Efficient Breast Lesion Segmentation Method for Medical Image Analysis</div><div class="text-base mb-1"><!--[--><div class=""><p><!--[-->Jiaming Liang, Teng Huang, Dan Li, Ziyu Ding, <strong><!--[-->Yunhao Li<!--]--></strong>, Lin Huang, Qiong Wang, Xi Zhang.<!--]--></p></div><!--]--></div><div class="text-base font-bold mb-2">Chinese Conference on Pattern Recognition and Computer Vision</div><div class="el-space el-space--horizontal mb-1 mt-1" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://link.springer.com/chapter/10.1007/978-981-99-8469-5_33" target="_blank"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><span class="el-tag el-tag--primary el-tag--light" style=""><span class="el-tag__content"><!--[-->PRCV<!--]--></span><!--v-if--></span><!--]--></div></div></div><!--]--></div></div><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><!--]--><!--]--></ul></div><div class="el-divider el-divider--horizontal" style="--el-border-style:solid;" role="separator"><!--v-if--></div><div><h1 class="text-2xl font-bold mb-4"> 💼 Work Experience </h1><ul class="el-timeline space-y-4"><!--[--><!--[--><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2023/10-2024/06</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><div class="el-card__header"><!--[--><div class="el-space el-space--horizontal" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><span class="el-avatar el-avatar--large el-avatar--square" style=""><img src="/imgs/china_post.png" style="object-fit:fill;"></span><!--]--></div><div class="el-space__item" style=""><!--[--><div class="flex flex-col"><span class="text-xl">ML Researcher &amp; Data Analyst</span><span class="text-l">China Post Bank</span></div><!--]--></div></div><!--]--></div><div class="el-card__body" style=""><!--[--><!--[--><div class="text-base"><ul><!--[--><li><!--[-->Developed AI-driven finance data precision recommendation model for customer marketing.<!--]--></li><li><!--[-->Designed and implemented a model training engine for automatic rolling training and optimization<!--]--></li><li><!--[-->Contributed to a 15% increase in performance for China Post Bank GuangZhou.<!--]--></li><!--]--></ul></div><!--]--><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><!--]--><!--]--></ul></div><div class="el-divider el-divider--horizontal" style="--el-border-style:solid;" role="separator"><!--v-if--></div><div><h1 class="text-2xl font-bold mb-4"> 💻 Programming Skills </h1><ul class="text-base list-disc"><li><b>AI &amp; Data</b>: PyTorch, Transformers, AutoGluon, Pandas, Matplotlib, OpenCV </li><li><b>Server</b>: Vue, FastAPI, Spring Cloud, MySQL, RabbitMQ, K8S, kubesphere, CI/CD </li><li><b>Contribute to</b>: <a href="https://github.com/run-llama/llama_index" target="_blank" class="text-inherit no-underline"><span>LlamaIndex <img src="https://img.shields.io/github/stars/run-llama/llama_index?style=flat" class="align-middle inline"></span></a>   <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="text-inherit no-underline"><span>LLaMA-Factory <img src="https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=flat" class="align-middle inline"></span></a>   <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" target="_blank" class="text-inherit no-underline"><span>Chinese-LLaMA-Alpaca <img src="https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca?style=flat" class="align-middle inline"></span></a></li></ul></div><div class="el-divider el-divider--horizontal" style="--el-border-style:solid;" role="separator"><!--v-if--></div><div><h1 class="text-2xl font-bold mb-4"> 🎓 Education </h1><ul class="text-base list-disc"><li> 2022-2025: Master of Artificial Intelligence, GuangZhou University, Advisor: <a class="text-[--el-color-primary]" href="https://pangyan.me/">Yan Pang </a>, GPA 3.58/4 (National Scholarship, Outstanding School Graduates) </li><li> 2018-2022: Bachelor in Computer Science, Guilin University of Electronic Technology, GPA 4.15/5 </li></ul></div><div class="el-divider el-divider--horizontal" style="--el-border-style:solid;" role="separator"><!--v-if--></div><div><h1 class="text-2xl font-bold mb-4"> 🏆 Honors &amp; Awards </h1><ul class="el-timeline space-y-4"><!--[--><!--[--><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2023</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><div class="el-card__header"><!--[--><div class="el-space el-space--horizontal" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><span class="el-avatar el-avatar--large el-avatar--square" style=""><img src="/imgs/pazhou.png" style="object-fit:fill;"></span><!--]--></div><div class="el-space__item" style=""><!--[--><div class="text-xl">First Prize in the National Guangzhou Pazhou Algorithm Competition</div><!--]--></div></div><!--]--></div><div class="el-card__body" style=""><!--[--><!--[--><div class="text-base"><ul><!--[--><li><!--[-->Lead AI and backend teams develop an Artificial Intelligence Security Assessment platform.<!--]--></li><li><!--[-->Implemented 20 + adversarial attack, 10 + defense algorithms, integrated image &amp; text classification, object detection models.<!--]--></li><!--]--></ul></div><!--]--><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2020</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><div class="el-card__header"><!--[--><div class="el-space el-space--horizontal" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><span class="el-avatar el-avatar--large el-avatar--square" style=""><img src="imgs/cccc.png" style="object-fit:fill;"></span><!--]--></div><div class="el-space__item" style=""><!--[--><div class="text-xl">Third Prize in the National China Collegiate Computing Contest-AI Track</div><!--]--></div></div><!--]--></div><div class="el-card__body" style=""><!--[--><!--[--><div class="text-base"><ul><!--[--><li><!--[-->Developed an Intelligent Construction Site Monitoring System for Helmet Detection and Tracking.<!--]--></li><li><!--[-->Implemented Helmet Detection and Tracking Pipeline by Vue, FastAPI, and YOLO.<!--]--></li><!--]--></ul></div><!--]--><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2020</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><div class="el-card__header"><!--[--><div class="el-space el-space--horizontal" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><span class="el-avatar el-avatar--large el-avatar--square" style=""><img src="imgs/csc.png" style="object-fit:fill;"></span><!--]--></div><div class="el-space__item" style=""><!--[--><div class="text-xl">Second Prize in the National 9th &quot;China Software Cup&quot;</div><!--]--></div></div><!--]--></div><div class="el-card__body" style=""><!--[--><!--[--><div class="text-base"><ul><!--[--><li><!--[-->Developed a Real-Time Vehicle Recognition and Tracking System for Traffic Light Scenarios.<!--]--></li><li><!--[-->Implemented Traffic Light Visual Processing Pipeline by YOLO, DeepTrack, and Vue, tracking cars and pedestrians.<!--]--></li><!--]--></ul></div><!--]--><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2020</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><div class="el-card__header"><!--[--><div class="el-space el-space--horizontal" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><span class="el-avatar el-avatar--large el-avatar--square" style=""><img src="/imgs/RoboMaster.png" style="object-fit:fill;"></span><!--]--></div><div class="el-space__item" style=""><!--[--><div class="text-xl">Second Prize in the National Competition at the RoboMaster</div><!--]--></div></div><!--]--></div><div class="el-card__body" style=""><!--[--><!--[--><div class="text-base"><ul><!--[--><li><!--[-->Led computer vision algorithm team to build real-time target recognition and tracking system for edge devices.<!--]--></li><li><!--[-->Co-work with the Mechanical Design and Circuit Design Department to build robots.<!--]--></li><!--]--></ul></div><!--]--><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><li class="el-timeline-item"><div class="el-timeline-item__tail"></div><div class="el-timeline-item__node el-timeline-item__node--normal" style="background-color:;"><!--v-if--></div><!--v-if--><div class="el-timeline-item__wrapper"><div class="el-timeline-item__timestamp is-top">2019</div><div class="el-timeline-item__content"><!--[--><div class="el-card is-always-shadow"><div class="el-card__header"><!--[--><div class="el-space el-space--horizontal" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><span class="el-avatar el-avatar--large el-avatar--square" style=""><img src="/imgs/ercc.jpg" style="object-fit:fill;"></span><!--]--></div><div class="el-space__item" style=""><!--[--><div class="text-xl">Second Prize in the National 9th &quot;China Education Robot Competition&quot;</div><!--]--></div></div><!--]--></div><div class="el-card__body" style=""><!--[--><!--[--><div class="text-base"><ul><!--[--><li><!--[-->Developed computer vision systems enable robots to recognize and navigate around obstacles and identify specific markers accurately.<!--]--></li><li><!--[-->Train object detection models that run on edge devices.<!--]--></li><!--]--></ul></div><!--]--><!--]--></div><!--v-if--></div><!--]--></div><!--v-if--></div></li><!--]--><!--]--></ul><h1 class="text-2xl font-bold mb-4"> 📜 Patent </h1><div class="el-collapse el-collapse-icon-position-right"><!--[--><div class="el-collapse-item"><div id="el-collapse-head-0" class="el-collapse-item__header" aria-expanded="false" aria-controls="el-collapse-content-0" aria-describedby="el-collapse-content-0" tabindex="0" role="button"><span class="el-collapse-item__title"><!--[-->Toggle to expand<!--]--></span><!--[--><i class="el-icon el-collapse-item__arrow" style=""><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path fill="currentColor" d="M340.864 149.312a30.592 30.592 0 0 0 0 42.752L652.736 512 340.864 831.872a30.592 30.592 0 0 0 0 42.752 29.12 29.12 0 0 0 41.728 0L714.24 534.336a32 32 0 0 0 0-44.672L382.592 149.376a29.12 29.12 0 0 0-41.728 0z"></path></svg><!--]--></i><!--]--></div><div id="el-collapse-content-0" role="region" class="el-collapse-item__wrap" aria-hidden="true" aria-labelledby="el-collapse-head-0" style="display:none;"><div class="el-collapse-item__content"><!--[--><ul class="text-base list-disc"><!--[--><li>2024, A medical image segmentation method, device, and medium <a href="https://patents.google.com/patent/CN118552553A" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[--><strong><!--[-->Yunhao Li<!--]--></strong>, Jiaming Liang, Junming Yan, Yan Pang, Changyu Dong, Teng Huang, Zheng Zhang.<!--]--></p></div><!--]--></li><li>2024, Method, device, apparatus, and medium for rapid segmentation of breast lesion features in medical video <a href="https://patents.google.com/patent/CN118537769A" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[-->JiamingLiang, Yan Pang, Teng Hunag, Ziyu Ding, Zhenyu Lu, <strong><!--[-->Yunhao Li<!--]--></strong>, Weiqing Kong, Wang Yang.<!--]--></p></div><!--]--></li><li>2024, Self-distilling and self-learning medical image segmentation method, device, and storage medium <a href="https://patents.google.com/patent/CN118587232A" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[-->Yan Pang, Changyu Dong, Teng Huang, <strong><!--[-->Yunhao Li<!--]--></strong>, Jiahui Huang, Hui Li, Caiyan Tang.<!--]--></p></div><!--]--></li><li>2022, A Deep Learning-Based Method and System for Satellite Anomaly Traffic Detection <a href="https://patents.google.com/patent/CN115834145A" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[-->HongYang Yan, Cong Wang, <strong><!--[-->Yunhao Li<!--]--></strong>, Weichuan Mo, Cong Li, Haiyang Wang, Yu Wang, Teng Huang<!--]--></p></div><!--]--></li><li>2022, A Network Security-Based Low Earth OrbitSatellite Simulation System <a href="https://patents.google.com/patent/CN115765842A" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[-->HongYang Yan, Bo Li, Cong Li, Haiyang Wang, <strong><!--[-->Yunhao Li<!--]--></strong>, Weichuan Mo, Yu Wang, Teng Huang<!--]--></p></div><!--]--></li><li>2021, A Panoptic Segmentation Method Based on Multi-Scale Edge Attention <a href="https://patents.google.com/patent/CN112802038A" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[-->Xiaochun Lei, Zhiying Liang, Zetao Jiang, Dingjie Zhang,<strong><!--[-->Yunhao Li<!--]--></strong>, Xiaolong Wang, Huiying Chen<!--]--></p></div><!--]--></li><li>2020, Intelligent Traffic Signal System <a href="https://patents.google.com/patent/CN213276964U" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[--><strong><!--[-->Yunhao Li<!--]--></strong>, Xiaochun Lei, Dingjie Zhang, Ziyuan Jing, Zhiying Liang, Yunyan Chen, Xiaolong Wang<!--]--></p></div><!--]--></li><li>2020, A Panoramic Segmentation Method Based on Edge Scaling Correction <a href="https://patents.google.com/patent/CN112489064B" rel="noopener noreferrer" target="_blank" class="link-primary" title="PDF"><span class="iconify i-lsicon:file-pdf-outline c-[--el-color-primary]" aria-hidden="true" style="font-size:18px;"></span></a><!--[--><div class=""><p><!--[-->Xiaochun Lei, Dingjie Zhang, Zetao Jiang, <strong><!--[-->Yunhao Li<!--]--></strong>, Yunyan Chen, Zhiying Liang, Huiying Chen<!--]--></p></div><!--]--></li><!--]--></ul><!--]--></div></div></div><!--]--></div></div><div class="el-divider el-divider--horizontal" style="--el-border-style:solid;" role="separator"><!--v-if--></div><div><div class="text-4xl text-center"> Yunhao Li </div><br><div class="text-center"><div class="el-space el-space--horizontal" style="align-items:center;row-gap:0px;column-gap:8px;"><div class="el-space__item" style=""><!--[--><a href="https://github.com/yunhaoli24" target="_blank" rel="noopener noreferrer"><span class="iconify i-garden:github-fill-16 c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://scholar.google.com/citations?user=TDnI7j8AAAAJ" target="_blank" rel="noopener noreferrer"><span class="iconify i-fa6-brands:google-scholar c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="mailto: li.yunhao@foxmail.com" target="_blank" rel="noopener noreferrer"><span class="iconify i-material-symbols:mail c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="https://orcid.org/0000-0003-1704-3050" target="_blank" rel="noopener noreferrer"><span class="iconify i-fa6-brands:orcid c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div><div class="el-space__item" style=""><!--[--><a href="/blog" target="_blank" rel="noopener noreferrer"><span class="iconify i-fa6-solid:blog c-[--el-color-primary]" aria-hidden="true" style="font-size:20px;"></span></a><!--]--></div></div></div><div class="mt-2 text-center"><p> © 2018 - 2025 Yunhao Li All Rights Reserved.<br></p></div></div></div></div><!--]--><!--]--></div><!--]--></div></div><!--[--><div class="el-scrollbar__bar is-horizontal" style="display:none;"><div class="el-scrollbar__thumb" style="width:;transform:translateX(0%);"></div></div><div class="el-scrollbar__bar is-vertical" style="display:none;"><div class="el-scrollbar__thumb" style="height:;transform:translateY(0%);"></div></div><!--]--></div><!--]--></main><!--]--></section></div></div></div></div><div id="teleports"></div><script type="application/ld+json" data-nuxt-schema-org="true" data-hid="schema-org-graph">{"@context":"https://schema.org","@graph":[{"@id":"https://yunhaoli.top/#website","@type":"WebSite","description":"Yunhao Li Blog","inLanguage":"en","name":"Yunhao Li","url":"https://yunhaoli.top/"},{"@id":"https://yunhaoli.top/#webpage","@type":"WebPage","description":"Blog of Yunhao Li, a passionate researcher who love to use programming skills and creativity to solve problems.","url":"https://yunhaoli.top/","isPartOf":{"@id":"https://yunhaoli.top/#website"},"potentialAction":[{"@type":"ReadAction","target":["https://yunhaoli.top/"]}]}]}</script><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__">[["ShallowReactive",1],{"prerenderedAt":2,"data":3,"state":411,"once":426,"_errors":427,"serverRendered":415,"path":429,"pinia":430},1757124435632,["ShallowReactive",4],{"search":5,"mdc--4f0gji-key":66,"mdc-1ckt63-key":87,"mdc--w4g90l-key":101,"mdc-t01z2g-key":115,"mdc-1z0vcq-key":128,"mdc--jxo5t-key":141,"mdc-coifyz-key":156,"mdc--q6hkun-key":171,"mdc--ikg6ps-key":186,"mdc--k2cy1-key":210,"mdc--jxzzzf-key":227,"mdc-j8ayvf-key":244,"mdc--xoghmn-key":261,"mdc--m8hd7v-key":278,"mdc-mdxesz-key":295,"mdc--j623yw-key":308,"mdc-eh3c9i-key":323,"mdc-oqnm4e-key":338,"mdc-xoon42-key":353,"mdc-yg28qq-key":368,"mdc-648q1h-key":383,"mdc--qhltya-key":396},[6,11,17,23,28,33,38,42,47,52,57,62],{"id":7,"title":8,"titles":9,"content":8,"level":10},"/blog/cuda-docker","构建PyTorch Docker镜像",[],1,{"id":12,"title":13,"titles":14,"content":15,"level":16},"/blog/cuda-docker#背景","背景",[8],"实验室服务器的内核版本较旧，某个项目需要升级内核，但由于服务器上运行着大量关键服务，直接升级可能存在风险。 因此，计划通过 Docker 构建一个包含 CUDA 和 Python 环境的镜像，通过 SSH 连接到容器中进行深度学习模型训练。",2,{"id":18,"title":19,"titles":20,"content":21,"level":22},"/blog/cuda-docker#需求","需求",[8,13],"CUDA 是深度学习模型训练的核心依赖，必须包含在镜像中。此外，SSH 服务也需要配置以支持远程访问。为了简化环境管理，计划将 Python 环境及常用依赖包一并打包到镜像中。 未来，任何需要进行深度学习模型训练的用户，只需启动一个容器并映射相应端口，即可获得完整的训练环境。",3,{"id":24,"title":25,"titles":26,"content":27,"level":16},"/blog/cuda-docker#安装","安装",[8],"",{"id":29,"title":30,"titles":31,"content":32,"level":22},"/blog/cuda-docker#宿主机安装-cuda-驱动","宿主机安装 CUDA 驱动",[8,25],"以下命令用于安装 NVIDIA 驱动程序（可根据需求选择其他版本）： apt install -y nvidia-driver-550 # 可根据实际需求更换版本",{"id":34,"title":35,"titles":36,"content":37,"level":22},"/blog/cuda-docker#宿主机安装-nvidia-容器工具包","宿主机安装 NVIDIA 容器工具包",[8,25],"访问 https://nvidia.github.io/nvidia-container-runtime/ 查看支持的操作系统和版本，并根据对应的选项添加软件源。以下步骤适用于 Ubuntu 系统： curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list 然后直接运行下面的命令就可以安装docker cuda驱动 sudo apt-get update\napt-get install -y nvidia-container-toolkit\ntee /etc/docker/daemon.json \u003C\u003C-'EOF'\n{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"/usr/bin/nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\nEOF\nsudo systemctl restart docker\nsudo nvidia-ctk runtime configure --runtime=containerd 进行测试，如果能成功出现显卡信息就可以了 docker run -it --rm --gpus all ubuntu nvidia-smi",{"id":39,"title":40,"titles":41,"content":27,"level":16},"/blog/cuda-docker#构建容器","构建容器",[8],{"id":43,"title":44,"titles":45,"content":46,"level":22},"/blog/cuda-docker#编写-dockerfile","编写 Dockerfile",[8,40],"将以下 Dockerfile 复制并根据需求进行调整。如果需要适配不同的 CUDA 版本，请参考 https://hub.docker.com/r/pytorch/pytorch/tags 修改 FROM 部分的版本号。 该镜像集成了图形界面所需的依赖包，并启用了 SSH 服务。通过环境变量设置了初始密码，便于管理员批量创建容器，无需手动进入容器设置密码。 FROM pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel\nLABEL author=\"li.yunhao@foxmail.com\"\n\nENV PASSWORD=\"123456\"\n\nRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && echo 'Asia/Shanghai' >/etc/timezone \\\n    && conda config --set show_channel_urls yes && conda init\\\n    && apt clean && apt update && apt install -y --no-install-recommends sudo \\\n    && apt install -y --no-install-recommends psmisc iputils-ping git-lfs libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev openssh-server git wget curl vim screen unzip build-essential cmake libncurses5-dev libncursesw5-dev pkg-config libdrm-dev libgtest-dev libudev-dev \\\n    && sed -i -e '$a\\PermitRootLogin yes' -e '$a\\PasswordAuthentication yes' /etc/ssh/sshd_config \\\n    && echo \"root:${PASSWORD}\" | chpasswd \\\n    && mkdir -p /run/sshd \\\n    && conda init \\\n    && apt clean\n\n# 安装nvtop\nRUN git clone https://github.com/Syllo/nvtop.git /opt/nvtop \\\n    && mkdir -p /opt/nvtop/build \\\n    && cd /opt/nvtop/build \\\n    && cmake .. \\\n    && make && sudo make install \\\n    && rm -r /opt/nvtop\n\nENTRYPOINT echo \"root:${PASSWORD}\" | chpasswd && /usr/sbin/sshd -D",{"id":48,"title":49,"titles":50,"content":51,"level":22},"/blog/cuda-docker#build容器","build容器",[8,40],"把Dockerfile写好之后就可以build了，可以按照希求更改build tag。 sudo docker build -t server/pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel .",{"id":53,"title":54,"titles":55,"content":56,"level":22},"/blog/cuda-docker#运行容器","运行容器",[8,40],"在启动docker容器的时候要注意加一些cuda的参数 -p是映射端口，我这里把50000端口映射出来供ssh使用--gpus all和-e NVIDIA_VISIBLE_DEVICES=all选择这个容器可见的显卡，如果需要控制可见显卡，可以通过--gpus 0,1 -e NVIDIA_VISIBLE_DEVICES=0,1指定。-e PASSWORD=\"your_container_password\"配置了容器的ssh密码 sudo docker run -itd \\\n    -p 50000:22 --name your_container_name \\\n    -e PASSWORD=\"your_container_password\" \\\n    --shm-size=128g \\\n    --gpus all -e NVIDIA_VISIBLE_DEVICES=all \\\n    --restart=always \\\n    server/pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel 运行容器之后就可以愉快的ssh连进去炼丹了，再也不用担心环境搞崩影响其他人了。 html pre.shiki code .srTi1, html code.shiki .srTi1{--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E}html pre.shiki code .sstjo, html code.shiki .sstjo{--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#E6DB74}html pre.shiki code .s7F3e, html code.shiki .s7F3e{--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#AE81FF}html pre.shiki code .s8-w5, html code.shiki .s8-w5{--shiki-default:#6A737D;--shiki-dark:#6A737D;--shiki-sepia:#88846F}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html.sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html pre.shiki code .sC2Qs, html code.shiki .sC2Qs{--shiki-default:#D73A49;--shiki-dark:#F97583;--shiki-sepia:#F92672}html pre.shiki code .sMOD_, html code.shiki .sMOD_{--shiki-default:#24292E;--shiki-dark:#E1E4E8;--shiki-sepia:#F8F8F2}html pre.shiki code .sinWB, html code.shiki .sinWB{--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#F8F8F2}",{"id":58,"title":59,"titles":60,"content":61,"level":10},"/blog","YunhaoLi's Blog",[],"欢迎访问我的博客！这里记录了我在深度学习以及其他领域的实践经验与心得。",{"id":63,"title":59,"titles":64,"content":65,"level":10},"/blog#yunhaolis-blog",[],"欢迎访问我的博客！这里记录了我在深度学习以及其他领域的实践经验与心得。 构建 CUDA Docker 镜像：详细介绍如何通过 Docker 构建包含 CUDA 环境的镜像，适用于深度学习模型训练。",{"data":67,"body":68},{},{"type":69,"children":70},"root",[71],{"type":72,"tag":73,"props":74,"children":75},"element","p",{},[76,79,85],{"type":77,"value":78},"text","Yan Pang, ",{"type":72,"tag":80,"props":81,"children":82},"strong",{},[83],{"type":77,"value":84},"Yunhao Li",{"type":77,"value":86},", Jiaming Liang, Hao Chen, Ying Hu, Qiong Wang.",{"data":88,"body":89},{},{"type":69,"children":90},[91],{"type":72,"tag":73,"props":92,"children":93},{},[94,95,99],{"type":77,"value":78},{"type":72,"tag":80,"props":96,"children":97},{},[98],{"type":77,"value":84},{"type":77,"value":100},", Teng Huang, Jiaming Liang, Ziyu Ding, Hao Chen, Baoliang Zhao, Ying Hu, Zheng Zhang, Qiong Wang.",{"data":102,"body":103},{},{"type":69,"children":104},[105],{"type":72,"tag":73,"props":106,"children":107},{},[108,109,113],{"type":77,"value":78},{"type":72,"tag":80,"props":110,"children":111},{},[112],{"type":77,"value":84},{"type":77,"value":114},", Teng Huang, Jiaming Liang, Zhen Wang, Changyu Dong, Dongyang Kuang, Ying Hu, Hao Chen, Tim Lei, Qiong Wang.",{"data":116,"body":117},{},{"type":69,"children":118},[119],{"type":72,"tag":73,"props":120,"children":121},{},[122,126],{"type":72,"tag":80,"props":123,"children":124},{},[125],{"type":77,"value":84},{"type":77,"value":127},", Chen Zibin, Yan Junming, Ding Ziyu, Li Jie ,Pei Xiaoqing, Zhang Zheng, Qiong Wang, Yan Pang.",{"data":129,"body":130},{},{"type":69,"children":131},[132],{"type":72,"tag":73,"props":133,"children":134},{},[135,139],{"type":72,"tag":80,"props":136,"children":137},{},[138],{"type":77,"value":84},{"type":77,"value":140},", Caiyan Tan, Mingdu Zhang, Xi Zhang, Teng Huang ,Xiao-Qing Pei, Yan Pang.",{"data":142,"body":143},{},{"type":69,"children":144},[145],{"type":72,"tag":73,"props":146,"children":147},{},[148,150,154],{"type":77,"value":149},"Yan Pang, Jiaming Liang, Teng Huang, Hao Chen, ",{"type":72,"tag":80,"props":151,"children":152},{},[153],{"type":77,"value":84},{"type":77,"value":155},", Dan Li, Lin Huang, Qiong Wang.",{"data":157,"body":158},{},{"type":69,"children":159},[160],{"type":72,"tag":73,"props":161,"children":162},{},[163,165,169],{"type":77,"value":164},"Bo Wei, Teng Huang, Xi Zhang, Jiaming Liang, ",{"type":72,"tag":80,"props":166,"children":167},{},[168],{"type":77,"value":84},{"type":77,"value":170},", Cong Cao, Dan Li, Yongfeng Chen, Huagang Xiong, Feng Jiang, Xiqiu Zhang.",{"data":172,"body":173},{},{"type":69,"children":174},[175],{"type":72,"tag":73,"props":176,"children":177},{},[178,180,184],{"type":77,"value":179},"Jiaming Liang, Teng Huang, Dan Li, Ziyu Ding, ",{"type":72,"tag":80,"props":181,"children":182},{},[183],{"type":77,"value":84},{"type":77,"value":185},", Lin Huang, Qiong Wang, Xi Zhang.",{"data":187,"body":188},{},{"type":69,"children":189},[190],{"type":72,"tag":191,"props":192,"children":193},"ul",{},[194,200,205],{"type":72,"tag":195,"props":196,"children":197},"li",{},[198],{"type":77,"value":199},"Developed AI-driven finance data precision recommendation model for customer marketing.",{"type":72,"tag":195,"props":201,"children":202},{},[203],{"type":77,"value":204},"Designed and implemented a model training engine for automatic rolling training and optimization",{"type":72,"tag":195,"props":206,"children":207},{},[208],{"type":77,"value":209},"Contributed to a 15% increase in performance for China Post Bank GuangZhou.",{"data":211,"body":212},{},{"type":69,"children":213},[214],{"type":72,"tag":191,"props":215,"children":216},{},[217,222],{"type":72,"tag":195,"props":218,"children":219},{},[220],{"type":77,"value":221},"Lead AI and backend teams develop an Artificial Intelligence Security Assessment platform.",{"type":72,"tag":195,"props":223,"children":224},{},[225],{"type":77,"value":226},"Implemented 20 + adversarial attack, 10 + defense algorithms, integrated image & text classification, object detection models.",{"data":228,"body":229},{},{"type":69,"children":230},[231],{"type":72,"tag":191,"props":232,"children":233},{},[234,239],{"type":72,"tag":195,"props":235,"children":236},{},[237],{"type":77,"value":238},"Developed an Intelligent Construction Site Monitoring System for Helmet Detection and Tracking.",{"type":72,"tag":195,"props":240,"children":241},{},[242],{"type":77,"value":243},"Implemented Helmet Detection and Tracking Pipeline by Vue, FastAPI, and YOLO.",{"data":245,"body":246},{},{"type":69,"children":247},[248],{"type":72,"tag":191,"props":249,"children":250},{},[251,256],{"type":72,"tag":195,"props":252,"children":253},{},[254],{"type":77,"value":255},"Developed a Real-Time Vehicle Recognition and Tracking System for Traffic Light Scenarios.",{"type":72,"tag":195,"props":257,"children":258},{},[259],{"type":77,"value":260},"Implemented Traffic Light Visual Processing Pipeline by YOLO, DeepTrack, and Vue, tracking cars and pedestrians.",{"data":262,"body":263},{},{"type":69,"children":264},[265],{"type":72,"tag":191,"props":266,"children":267},{},[268,273],{"type":72,"tag":195,"props":269,"children":270},{},[271],{"type":77,"value":272},"Led computer vision algorithm team to build real-time target recognition and tracking system for edge devices.",{"type":72,"tag":195,"props":274,"children":275},{},[276],{"type":77,"value":277},"Co-work with the Mechanical Design and Circuit Design Department to build robots.",{"data":279,"body":280},{},{"type":69,"children":281},[282],{"type":72,"tag":191,"props":283,"children":284},{},[285,290],{"type":72,"tag":195,"props":286,"children":287},{},[288],{"type":77,"value":289},"Developed computer vision systems enable robots to recognize and navigate around obstacles and identify specific markers accurately.",{"type":72,"tag":195,"props":291,"children":292},{},[293],{"type":77,"value":294},"Train object detection models that run on edge devices.",{"data":296,"body":297},{},{"type":69,"children":298},[299],{"type":72,"tag":73,"props":300,"children":301},{},[302,306],{"type":72,"tag":80,"props":303,"children":304},{},[305],{"type":77,"value":84},{"type":77,"value":307},", Jiaming Liang, Junming Yan, Yan Pang, Changyu Dong, Teng Huang, Zheng Zhang.",{"data":309,"body":310},{},{"type":69,"children":311},[312],{"type":72,"tag":73,"props":313,"children":314},{},[315,317,321],{"type":77,"value":316},"JiamingLiang, Yan Pang, Teng Hunag, Ziyu Ding, Zhenyu Lu, ",{"type":72,"tag":80,"props":318,"children":319},{},[320],{"type":77,"value":84},{"type":77,"value":322},", Weiqing Kong, Wang Yang.",{"data":324,"body":325},{},{"type":69,"children":326},[327],{"type":72,"tag":73,"props":328,"children":329},{},[330,332,336],{"type":77,"value":331},"Yan Pang, Changyu Dong, Teng Huang, ",{"type":72,"tag":80,"props":333,"children":334},{},[335],{"type":77,"value":84},{"type":77,"value":337},", Jiahui Huang, Hui Li, Caiyan Tang.",{"data":339,"body":340},{},{"type":69,"children":341},[342],{"type":72,"tag":73,"props":343,"children":344},{},[345,347,351],{"type":77,"value":346},"HongYang Yan, Cong Wang, ",{"type":72,"tag":80,"props":348,"children":349},{},[350],{"type":77,"value":84},{"type":77,"value":352},", Weichuan Mo, Cong Li, Haiyang Wang, Yu Wang, Teng Huang",{"data":354,"body":355},{},{"type":69,"children":356},[357],{"type":72,"tag":73,"props":358,"children":359},{},[360,362,366],{"type":77,"value":361},"HongYang Yan, Bo Li, Cong Li, Haiyang Wang, ",{"type":72,"tag":80,"props":363,"children":364},{},[365],{"type":77,"value":84},{"type":77,"value":367},", Weichuan Mo, Yu Wang, Teng Huang",{"data":369,"body":370},{},{"type":69,"children":371},[372],{"type":72,"tag":73,"props":373,"children":374},{},[375,377,381],{"type":77,"value":376},"Xiaochun Lei, Zhiying Liang, Zetao Jiang, Dingjie Zhang,",{"type":72,"tag":80,"props":378,"children":379},{},[380],{"type":77,"value":84},{"type":77,"value":382},", Xiaolong Wang, Huiying Chen",{"data":384,"body":385},{},{"type":69,"children":386},[387],{"type":72,"tag":73,"props":388,"children":389},{},[390,394],{"type":72,"tag":80,"props":391,"children":392},{},[393],{"type":77,"value":84},{"type":77,"value":395},", Xiaochun Lei, Dingjie Zhang, Ziyuan Jing, Zhiying Liang, Yunyan Chen, Xiaolong Wang",{"data":397,"body":398},{},{"type":69,"children":399},[400],{"type":72,"tag":73,"props":401,"children":402},{},[403,405,409],{"type":77,"value":404},"Xiaochun Lei, Dingjie Zhang, Zetao Jiang, ",{"type":72,"tag":80,"props":406,"children":407},{},[408],{"type":77,"value":84},{"type":77,"value":410},", Yunyan Chen, Zhiying Liang, Huiying Chen",["Reactive",412],{"$scolor-mode":413,"$snuxt-seo-utils:routeRules":417,"$ssite-config":418},{"preference":414,"value":414,"unknown":415,"forced":416},"system",true,false,{"head":-1,"seoMeta":-1},{"_priority":419,"description":423,"env":424,"name":84,"url":425},{"name":420,"env":421,"description":422,"url":420},-3,-15,-10,"Yunhao Li Blog","production","https://yunhaoli.top",["Set"],["ShallowReactive",428],{"search":-1,"mdc--4f0gji-key":-1,"mdc-1ckt63-key":-1,"mdc--w4g90l-key":-1,"mdc-t01z2g-key":-1,"mdc-1z0vcq-key":-1,"mdc--jxo5t-key":-1,"mdc-coifyz-key":-1,"mdc--q6hkun-key":-1,"mdc--ikg6ps-key":-1,"mdc--k2cy1-key":-1,"mdc--jxzzzf-key":-1,"mdc-j8ayvf-key":-1,"mdc--xoghmn-key":-1,"mdc--m8hd7v-key":-1,"mdc-mdxesz-key":-1,"mdc--j623yw-key":-1,"mdc-eh3c9i-key":-1,"mdc-oqnm4e-key":-1,"mdc-xoon42-key":-1,"mdc-yg28qq-key":-1,"mdc-648q1h-key":-1,"mdc--qhltya-key":-1},"/",{}]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{"seo-utils":{canonicalQueryWhitelist:["page","sort","filter","search","q","category","tag"],canonicalLowercase:true},device:{defaultUserAgent:"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.39 Safari/537.36",enabled:true,refreshOnResize:false},content:{wsUrl:""},mdc:{components:{prose:true,map:{}},headings:{anchorLinks:{h1:false,h2:false,h3:false,h4:false,h5:false,h6:false}}}},app:{baseURL:"/",buildId:"bb85fc13-cd4e-4b6a-a07d-f81f8017cc1b",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>